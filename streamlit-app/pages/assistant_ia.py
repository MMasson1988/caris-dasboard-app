"""
Page Assistant IA - Chatbot MEAL bas√© sur Gemini 2.0 Flash
"""
import streamlit as st
import pandas as pd

# Imports locaux
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

from utils.data_loader import load_depistage, load_enrolled, get_data_summary
from utils.kpi_calculator import calculate_kpis
from utils.ai_chatbot import (
    query_gemini, build_meal_context, get_suggested_questions,
    initialize_chat_history, add_to_chat_history, clear_chat_history,
    get_gemini_client
)


def render_assistant():
    """Affiche la page de l'assistant IA MEAL."""
    
    st.title("ü§ñ Assistant IA MEAL")
    st.markdown("---")
    
    # Description
    st.markdown("""
    <div style='background-color: #e3f2fd; padding: 15px; border-radius: 10px; border-left: 4px solid #2196f3; margin-bottom: 20px;'>
        <h4 style='margin: 0; color: #1565c0;'>üí° Assistant Analytique Intelligent</h4>
        <p style='margin: 10px 0 0 0;'>
            Posez vos questions sur les donn√©es nutrition en langage naturel. 
            L'assistant utilise <strong>Gemini 2.0 Flash</strong> pour analyser les donn√©es 
            et fournir des r√©ponses contextualis√©es selon le cadre MEAL.
        </p>
    </div>
    """, unsafe_allow_html=True)
    
    # V√©rifier la configuration Gemini
    model = get_gemini_client()
    
    if model is None:
        st.error("""
        ‚ö†Ô∏è **Configuration Gemini requise**
        
        Pour utiliser l'assistant IA, ajoutez votre cl√© API Gemini dans `.streamlit/secrets.toml`:
        
        ```toml
        [gemini]
        api_key = "votre_cl√©_api_gemini"
        ```
        
        Obtenez une cl√© API gratuite sur: [Google AI Studio](https://aistudio.google.com/app/apikey)
        """)
        
        # Mode d√©mo sans API
        st.markdown("---")
        st.markdown("### üìö Mode D√©monstration")
        st.info("L'assistant fonctionne en mode d√©monstration. Les r√©ponses sont des exemples statiques.")
        
    # Charger les donn√©es pour le contexte
    with st.spinner("Chargement du contexte analytique..."):
        df_depistage = load_depistage()
        df_enrolled = load_enrolled()
        kpis = calculate_kpis(df_depistage, df_enrolled, "current_month")
    
    # Sidebar avec informations contextuelles
    with st.sidebar:
        st.markdown("### üìä Contexte Charg√©")
        
        if not df_depistage.empty:
            st.metric("D√©pistages", len(df_depistage))
        if not df_enrolled.empty:
            st.metric("Enr√¥l√©s", len(df_enrolled))
        
        st.markdown("---")
        
        # Bouton pour effacer l'historique
        if st.button("üóëÔ∏è Effacer la conversation", use_container_width=True):
            clear_chat_history()
            st.rerun()
    
    # Construire le contexte MEAL
    context = build_meal_context(df_depistage, df_enrolled, kpis)
    
    # Initialiser l'historique
    initialize_chat_history()
    
    # ============================================
    # INTERFACE DE CHAT
    # ============================================
    
    # Questions sugg√©r√©es
    st.markdown("### üí° Questions Sugg√©r√©es")
    
    suggested = get_suggested_questions()
    
    cols = st.columns(2)
    for i, question in enumerate(suggested[:6]):
        with cols[i % 2]:
            if st.button(f"üí¨ {question[:50]}...", key=f"suggest_{i}", use_container_width=True):
                st.session_state.pending_question = question
    
    st.markdown("---")
    
    # Afficher l'historique de conversation
    st.markdown("### üí¨ Conversation")
    
    chat_container = st.container()
    
    with chat_container:
        for message in st.session_state.get("chat_history", []):
            if message["role"] == "user":
                with st.chat_message("user", avatar="üë§"):
                    st.markdown(message["content"])
            else:
                with st.chat_message("assistant", avatar="ü§ñ"):
                    st.markdown(message["content"])
    
    # Zone de saisie
    st.markdown("---")
    
    # V√©rifier si une question sugg√©r√©e a √©t√© cliqu√©e
    if "pending_question" in st.session_state:
        user_input = st.session_state.pending_question
        del st.session_state.pending_question
    else:
        user_input = st.chat_input("Posez votre question sur les donn√©es nutrition...")
    
    if user_input:
        # Ajouter la question √† l'historique
        add_to_chat_history("user", user_input)
        
        # Afficher imm√©diatement la question
        with st.chat_message("user", avatar="üë§"):
            st.markdown(user_input)
        
        # G√©n√©rer la r√©ponse
        with st.spinner("üîÑ Analyse en cours..."):
            if model is not None:
                response = query_gemini(user_input, context, st.session_state.get("chat_history", []))
            else:
                # R√©ponse d√©mo
                response = generate_demo_response(user_input, kpis, df_depistage)
        
        # Ajouter la r√©ponse √† l'historique
        add_to_chat_history("assistant", response)
        
        # Afficher la r√©ponse
        with st.chat_message("assistant", avatar="ü§ñ"):
            st.markdown(response)
        
        # Recharger pour mettre √† jour l'historique complet
        st.rerun()
    
    # ============================================
    # INFORMATIONS SUPPL√âMENTAIRES
    # ============================================
    st.markdown("---")
    
    with st.expander("üìö Capacit√©s de l'Assistant"):
        st.markdown("""
        **L'assistant peut vous aider √†:**
        
        - üìä **Analyser les KPIs** : Interpr√©ter les indicateurs de performance
        - üîç **Explorer les donn√©es** : Identifier des patterns et tendances
        - üè¢ **Comparer les bureaux** : Performance relative entre zones
        - ‚ö†Ô∏è **D√©tecter les anomalies** : Cas n√©cessitant attention
        - üí° **Recommander des actions** : Suggestions bas√©es sur l'analyse
        
        **Limites:**
        - Analyse bas√©e uniquement sur les donn√©es charg√©es
        - Ne remplace pas l'expertise MEAL humaine
        - Peut n√©cessiter validation des calculs complexes
        """)
    
    with st.expander("üìã Donn√©es Disponibles"):
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**D√©pistages:**")
            if not df_depistage.empty:
                st.write(f"- Colonnes: {len(df_depistage.columns)}")
                st.write(f"- Enregistrements: {len(df_depistage)}")
                st.write(f"- Bureaux: {df_depistage['office'].nunique() if 'office' in df_depistage.columns else 'N/A'}")
        
        with col2:
            st.markdown("**Enr√¥lements:**")
            if not df_enrolled.empty:
                st.write(f"- Colonnes: {len(df_enrolled.columns)}")
                st.write(f"- Enregistrements: {len(df_enrolled)}")
                st.write(f"- Actifs: {len(df_enrolled[df_enrolled['actif'] == True]) if 'actif' in df_enrolled.columns else 'N/A'}")


def generate_demo_response(question: str, kpis: dict, df: pd.DataFrame) -> str:
    """
    G√©n√®re une r√©ponse de d√©monstration sans API Gemini.
    
    Args:
        question: Question de l'utilisateur
        kpis: KPIs calcul√©s
        df: DataFrame de d√©pistage
    
    Returns:
        R√©ponse format√©e
    """
    question_lower = question.lower()
    
    if "mas" in question_lower and "bureau" in question_lower:
        if not df.empty and 'office' in df.columns and 'manutrition_type' in df.columns:
            mas_by_office = df[df['manutrition_type'] == 'MAS'].groupby('office').size()
            if not mas_by_office.empty:
                top_office = mas_by_office.idxmax()
                top_count = mas_by_office.max()
                return f"""
**Analyse des cas MAS par bureau:**

Le bureau avec la plus forte proportion de cas MAS est **{top_office}** avec **{top_count} cas**.

**R√©partition compl√®te:**
{mas_by_office.to_string()}

**Recommandation:** 
Il serait pertinent d'investiguer les causes potentielles dans la zone de {top_office} 
et de renforcer les activit√©s de sensibilisation communautaire.
"""
        return "Donn√©es insuffisantes pour cette analyse. Veuillez v√©rifier que les donn√©es sont charg√©es."
    
    elif "tendance" in question_lower or "√©volution" in question_lower:
        return f"""
**Analyse des tendances:**

Bas√© sur les donn√©es actuelles:
- Total d√©pistages (mois courant): **{kpis.get('total_depistages', 0)}**
- Cas MAS: **{kpis.get('cas_mas', 0)}** ({kpis.get('proportion_mas', 0)}%)
- Cas MAM: **{kpis.get('cas_mam', 0)}** ({kpis.get('proportion_mam', 0)}%)

Pour une analyse de tendance compl√®te, consultez l'onglet "Tendances" du Dashboard 
qui affiche l'√©volution hebdomadaire et mensuelle.
"""
    
    elif "taux" in question_lower and "admission" in question_lower:
        return f"""
**Taux d'admission:**

Le taux d'admission global est de **{kpis.get('taux_admission', 0)}%**.

Cela signifie que sur {kpis.get('depistages_eligibles', 0)} enfants √©ligibles d√©pist√©s, 
{kpis.get('total_enrolled', 0)} ont √©t√© effectivement enr√¥l√©s dans le programme.

**Interpr√©tation:**
- Taux > 70%: Bonne performance
- Taux 40-70%: Performance moyenne, actions d'am√©lioration recommand√©es
- Taux < 40%: Performance insuffisante, investigation requise
"""
    
    else:
        return f"""
**R√©sum√© des donn√©es disponibles:**

üìä **KPIs du mois courant:**
- D√©pistages: {kpis.get('total_depistages', 0)}
- √âligibles: {kpis.get('depistages_eligibles', 0)}
- Enr√¥l√©s: {kpis.get('total_enrolled', 0)}
- Cas MAS: {kpis.get('cas_mas', 0)}
- Cas MAM: {kpis.get('cas_mam', 0)}
- Taux d'admission: {kpis.get('taux_admission', 0)}%

N'h√©sitez pas √† poser une question plus sp√©cifique sur ces donn√©es 
ou √† utiliser les questions sugg√©r√©es ci-dessus.

*Note: Mode d√©monstration actif. Pour des analyses avanc√©es, configurez l'API Gemini.*
"""
